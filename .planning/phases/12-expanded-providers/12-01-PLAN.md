---
phase: 12-expanded-providers
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - packages/cli/src/vault/providers.ts
  - packages/gateway/src/llm/types.ts
  - packages/gateway/src/llm/registry.ts
  - packages/gateway/src/llm/router-rules.ts
  - packages/gateway/src/usage/pricing.ts
  - packages/core/src/config/schema.ts
  - packages/gateway/package.json
autonomous: true
requirements:
  - "Venice API integration"
  - "Google AI Studio/Gemini integration"
  - "Ollama remote host configuration"
  - "Extended provider registry"

must_haves:
  truths:
    - "Venice AI provider is registered when a Venice API key is configured in the vault"
    - "Google Gemini provider is registered when a Google API key is configured in the vault"
    - "Ollama endpoint URL is configurable via ollamaEndpoints in app config"
    - "Multiple Ollama endpoints can be registered simultaneously with unique provider names"
    - "Provider-qualified model IDs work for venice:*, google:*, and ollama-*:* prefixes"
    - "Pricing lookups return correct rates for Venice and Gemini models"
  artifacts:
    - path: "packages/gateway/src/llm/registry.ts"
      provides: "Venice, Google, and configurable Ollama provider registration"
      contains: "createGoogleGenerativeAI"
    - path: "packages/cli/src/vault/providers.ts"
      provides: "Extended PROVIDERS array with venice and google"
      contains: "venice"
    - path: "packages/core/src/config/schema.ts"
      provides: "OllamaEndpoint schema for remote host configuration"
      contains: "ollamaEndpoints"
    - path: "packages/gateway/src/usage/pricing.ts"
      provides: "Venice and Gemini model pricing entries"
      contains: "venice:"
  key_links:
    - from: "packages/gateway/src/llm/registry.ts"
      to: "@ai-sdk/google"
      via: "createGoogleGenerativeAI import"
      pattern: "createGoogleGenerativeAI"
    - from: "packages/gateway/src/llm/registry.ts"
      to: "packages/cli/src/vault/providers.ts"
      via: "getKey('venice') and getKey('google')"
      pattern: "getKey.*venice|getKey.*google"
    - from: "packages/gateway/src/llm/registry.ts"
      to: "packages/core/src/config/schema.ts"
      via: "ollamaEndpoints config parameter"
      pattern: "ollamaEndpoints"
---

<objective>
Extend the provider registry with Venice AI, Google AI Studio (Gemini), and configurable Ollama remote endpoints.

Purpose: Users need to connect to Venice AI for text/image/video models, Google Gemini for its model lineup, and Ollama instances running on LAN/cloud hosts beyond localhost.

Output: Extended provider registry, vault provider list, config schema, and pricing table supporting all five providers.
</objective>

<execution_context>
@/Users/hitekmedia/.claude/get-shit-done/workflows/execute-plan.md
@/Users/hitekmedia/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/12-expanded-providers/12-RESEARCH.md
@.planning/phases/04-multi-provider-intelligence/04-01-SUMMARY.md

Key existing patterns:
- Provider registry: packages/gateway/src/llm/registry.ts — singleton buildRegistry() with lazy init, conditional registration per API key
- Provider list: packages/cli/src/vault/providers.ts — PROVIDERS const array, PROVIDER_KEY_PREFIXES, validateProvider()
- Config schema: packages/core/src/config/schema.ts — AppConfigSchema with Zod
- Pricing: packages/gateway/src/usage/pricing.ts — MODEL_PRICING record with fuzzy match and Ollama wildcard
- Types: packages/gateway/src/llm/types.ts — ProviderName union type
- Router rules: packages/gateway/src/llm/router-rules.ts — DEFAULT_TIERS mapping
</context>

<tasks>

<task type="auto">
  <name>Task 1: Extend provider types, vault, and config schema</name>
  <files>
    packages/cli/src/vault/providers.ts
    packages/gateway/src/llm/types.ts
    packages/core/src/config/schema.ts
  </files>
  <action>
1. In `packages/cli/src/vault/providers.ts`:
   - Add "venice" and "google" to the PROVIDERS const array (after "ollama")
   - Add venice: null and google: null to PROVIDER_KEY_PREFIXES (Venice uses bearer tokens with no standard prefix; Google AI Studio keys have variable format like AIzaSy...)
   - validateProvider() works automatically since it checks against PROVIDERS array

2. In `packages/gateway/src/llm/types.ts`:
   - Extend ProviderName union to: "anthropic" | "openai" | "ollama" | "venice" | "google"

3. In `packages/core/src/config/schema.ts`:
   - Add OllamaEndpointSchema: z.object({ name: z.string(), url: z.string().url() })
   - Export the OllamaEndpoint type
   - Add ollamaEndpoints: z.array(OllamaEndpointSchema).optional() to AppConfigSchema
  </action>
  <verify>
cd /Users/hitekmedia/Documents/GitHub/AgentSpace && npx tsc -p packages/core/tsconfig.json --noEmit && npx tsc -p packages/cli/tsconfig.json --noEmit
  </verify>
  <done>PROVIDERS includes "venice" and "google", ProviderName includes both new providers, AppConfigSchema accepts ollamaEndpoints array, all type-check passes</done>
</task>

<task type="auto">
  <name>Task 2: Extend provider registry, pricing, and routing tiers</name>
  <files>
    packages/gateway/src/llm/registry.ts
    packages/gateway/src/usage/pricing.ts
    packages/gateway/src/llm/router-rules.ts
    packages/gateway/package.json
  </files>
  <action>
1. Install @ai-sdk/google:
   ```bash
   cd packages/gateway && npm install @ai-sdk/google@^3
   ```

2. In `packages/gateway/src/llm/registry.ts`:
   - Import createGoogleGenerativeAI from "@ai-sdk/google"
   - Extend buildRegistry keys parameter to accept: venice?: string, google?: string, ollamaEndpoints?: Array<{ name: string; url: string }>
   - Add Venice registration (conditional on key): use createOpenAICompatible with name "venice", baseURL "https://api.venice.ai/api/v1", apiKey from vault getKey("venice")
   - Add Google registration (conditional on key): use createGoogleGenerativeAI({ apiKey }) with key from vault getKey("google")
   - Refactor Ollama registration: read ollamaEndpoints from keys parameter, default to [{ name: "localhost", url: "http://localhost:11434/v1" }]. When only one endpoint, use provider name "ollama". When multiple, use "ollama" for first and "ollama-{name}" for rest (backward compat).
   - Update getAvailableProviders() to include "venice" and "google" when their keys exist. For multiple Ollama endpoints, list each as separate provider.
   - Keep the cachedRegistry singleton pattern as-is.
   - Update getRegistry() to read ollamaEndpoints from config and pass into buildRegistry(): import loadConfig from "@agentspace/core", then call `const cfg = loadConfig();` and pass `buildRegistry({ ollamaEndpoints: cfg?.ollamaEndpoints ?? undefined })`. This wires the config field (added in Task 1) through to the registry function.

3. In `packages/gateway/src/usage/pricing.ts`:
   - Add Venice model pricing entries:
     "venice:llama-3.3-70b": { inputPerMTok: 0.4, outputPerMTok: 0.4 }
     "venice:deepseek-r1-671b": { inputPerMTok: 2.0, outputPerMTok: 2.0 }
     "venice:minimax-m1-80k": { inputPerMTok: 0.5, outputPerMTok: 0.5 }
   - Add Google Gemini pricing entries:
     "google:gemini-2.5-pro": { inputPerMTok: 1.25, outputPerMTok: 10 }
     "google:gemini-2.5-flash": { inputPerMTok: 0.15, outputPerMTok: 0.6 }
     "google:gemini-2.0-flash": { inputPerMTok: 0.1, outputPerMTok: 0.4 }
   - Add venice: wildcard in getModelPricing() (same pattern as ollama:), defaulting to { inputPerMTok: 0.5, outputPerMTok: 0.5 }

4. In `packages/gateway/src/llm/router-rules.ts`:
   - No changes to DEFAULT_TIERS needed yet (stays Anthropic). The routing system already works with any provider:model ID via msg.model bypass.
  </action>
  <verify>
cd /Users/hitekmedia/Documents/GitHub/AgentSpace && npx tsc -p packages/gateway/tsconfig.json --noEmit && node -e "const p = require('./packages/gateway/package.json'); console.log('@ai-sdk/google:', p.dependencies['@ai-sdk/google'])"
  </verify>
  <done>@ai-sdk/google installed, registry registers Venice/Google/configurable-Ollama providers, pricing returns correct values for venice:* and google:* models, TypeScript compiles cleanly</done>
</task>

</tasks>

<verification>
1. `npx tsc --noEmit` passes for core, cli, and gateway packages
2. PROVIDERS array contains exactly ["anthropic", "openai", "ollama", "venice", "google"]
3. buildRegistry() accepts venice, google keys and ollamaEndpoints parameter
4. getModelPricing("venice:llama-3.3-70b") returns { inputPerMTok: 0.4, outputPerMTok: 0.4 }
5. getModelPricing("google:gemini-2.5-pro") returns { inputPerMTok: 1.25, outputPerMTok: 10 }
6. getModelPricing("venice:unknown-model") returns Venice wildcard pricing (not Sonnet default)
</verification>

<success_criteria>
- Venice AI, Google Gemini, and configurable Ollama providers compile and register in the provider registry
- Provider-qualified model IDs resolve correctly for all five providers
- Pricing table covers Venice and Gemini models with fuzzy matching
- Ollama endpoints are configurable via app config instead of hardcoded localhost
- Existing Anthropic and OpenAI providers remain unaffected
</success_criteria>

<output>
After completion, create `.planning/phases/12-expanded-providers/12-01-SUMMARY.md`
</output>
